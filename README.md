# Analyzing and Mitigating Overfitting in Neural Networks
Part of MLP course University of Edinburgh.
## Description:
This project explores the persistent issue of overfitting in neural network training, particularly observing the impacts of network width and depth on model generalization. We employ and analyze mitigation techniques like Dropout and Weight Penalty on the EMNIST dataset, finding notable success with Dropout regularization. Additionally, we delve into the nuanced application of weight decay in adaptive gradient algorithms with the Adam optimizer, examining its intricate relationship with L2 Weight Penalty and formulating a hybrid implementation approach.
